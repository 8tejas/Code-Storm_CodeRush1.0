{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yXeOPG6TTdo",
        "outputId": "fab8f0fd-c6cb-4485-a27e-b063ca97de07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "replace Schizophrenia/norm/S10W1.eea? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the paths to your data folders using the correct folder name\n",
        "schizophrenia_dir = '/content/Schizophrenia/sch/'\n",
        "normal_dir = '/content/Schizophrenia/norm/'\n",
        "\n",
        "# Get a list of all file names in each folder\n",
        "schizophrenia_files = os.listdir(schizophrenia_dir)\n",
        "normal_files = os.listdir(normal_dir)\n",
        "\n",
        "print(f'Found {len(schizophrenia_files)} schizophrenia files.')\n",
        "print(f'Found {len(normal_files)} normal files.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7MQaltITXEI",
        "outputId": "48ef9bd0-27fb-4c16-cf4c-35f7d5716ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 45 schizophrenia files.\n",
            "Found 39 normal files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one file to begin the cleaning process\n",
        "file_path = '/content/Schizophrenia/norm/S10W1.eea'\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UuwDGTTUHpC",
        "outputId": "86dedc0a-5a6b-4af9-f7d6-bdf9b6ba3218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0\n",
            "0  347.78\n",
            "1  507.87\n",
            "2  488.54\n",
            "3  369.86\n",
            "4  347.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "agumenting the data to avoid overfitting"
      ],
      "metadata": {
        "id": "J97CRuzMhAUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def augment_eeg_data(data, noise_level=0.01):\n",
        "    \"\"\"\n",
        "    Augments the EEG data by adding Gaussian noise.\n",
        "    :param data: A NumPy array of your EEG data (e.g., all_data list from your code)\n",
        "    :param noise_level: The standard deviation of the noise to add\n",
        "    :return: An augmented list of EEG data arrays\n",
        "    \"\"\"\n",
        "    augmented_data = []\n",
        "\n",
        "    for subject_data in data:\n",
        "        # Create Gaussian noise with the same shape as the subject's data\n",
        "        noise = np.random.normal(0, noise_level, subject_data.shape)\n",
        "\n",
        "        # Add the noise to the original data\n",
        "        noisy_data = subject_data + noise\n",
        "\n",
        "        # Add both the original and augmented data to the new list\n",
        "        augmented_data.append(subject_data)\n",
        "        augmented_data.append(noisy_data)\n",
        "\n",
        "    return augmented_data"
      ],
      "metadata": {
        "id": "SWbYfQ8Yg_QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for single file the feature is extrated"
      ],
      "metadata": {
        "id": "Us2VvaPJZUHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the file\n",
        "file_path = '/content/Schizophrenia/norm/S10W1.eea'\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "# Convert to a NumPy array\n",
        "data_array = df.values\n",
        "\n",
        "# Define the number of channels\n",
        "n_channels = 16\n",
        "\n",
        "# Reshape the data using -1 to let NumPy figure out the number of samples\n",
        "reshaped_data = data_array.reshape(-1, n_channels)\n",
        "\n",
        "# Print the new shape to see the corrected number of samples\n",
        "print(\"Original shape:\", data_array.shape)\n",
        "print(\"Corrected reshaped shape:\", reshaped_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCNwok6EVKRc",
        "outputId": "5a529151-c2ba-4732-b5ca-3ad60ba08504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (122880, 1)\n",
            "Corrected reshaped shape: (7680, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for all the file the featured is extrated"
      ],
      "metadata": {
        "id": "pQ7JREJFZX9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the paths to your data folders\n",
        "schizophrenia_dir = '/content/Schizophrenia/sch/'\n",
        "normal_dir = '/content/Schizophrenia/norm/'\n",
        "\n",
        "# Define the number of channels\n",
        "n_channels = 16\n",
        "\n",
        "# Create empty lists to store all the reshaped data and labels\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "# --- Loop through all schizophrenia files ---\n",
        "for filename in os.listdir(schizophrenia_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(schizophrenia_dir, filename)\n",
        "\n",
        "        # Load the data\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "        # Reshape the data\n",
        "        data_array = df.values\n",
        "        reshaped_data = data_array.reshape(-1, n_channels)\n",
        "\n",
        "        # Add the reshaped data and its label to the lists\n",
        "        all_data.append(reshaped_data)\n",
        "        all_labels.append(1) # Label for schizophrenia (1)\n",
        "\n",
        "# --- Loop through all normal files ---\n",
        "for filename in os.listdir(normal_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(normal_dir, filename)\n",
        "\n",
        "        # Load the data\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "        # Reshape the data\n",
        "        data_array = df.values\n",
        "        reshaped_data = data_array.reshape(-1, n_channels)\n",
        "\n",
        "        # Add the reshaped data and its label to the lists\n",
        "        all_data.append(reshaped_data)\n",
        "        all_labels.append(0) # Label for normal (0)\n",
        "\n",
        "print(f\"Total files processed: {len(all_data)}\")\n",
        "print(f\"Total labels created: {len(all_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcX41gw3Za8u",
        "outputId": "d358a190-b525-48e3-eaca-b9e98565f704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files processed: 84\n",
            "Total labels created: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now extreding the features"
      ],
      "metadata": {
        "id": "4cXUXO_YZ1z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.fftpack import rfft, fftfreq\n",
        "import pywt\n",
        "\n",
        "# Your feature extraction function\n",
        "def extract_features(eeg_data, n_channels=16, sample_rate=128):\n",
        "    all_features = []\n",
        "    # Loop through each subject's data in the list\n",
        "    for subject_data in eeg_data:\n",
        "        subject_features = []\n",
        "        # Loop through each of the 16 channels\n",
        "        for channel_data in subject_data.T:\n",
        "\n",
        "            # --- Fourier Transform Features (Frequency Power) ---\n",
        "            N = len(channel_data)\n",
        "            yf = rfft(channel_data)\n",
        "            xf = fftfreq(N, 1 / sample_rate)\n",
        "            alpha_power = np.sum(np.abs(yf[(xf > 8) & (xf < 12)]))\n",
        "            subject_features.append(alpha_power)\n",
        "\n",
        "            # --- Wavelet Transform Features (Decomposition) ---\n",
        "            coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n",
        "            subject_features.extend(coeffs[0])\n",
        "\n",
        "        all_features.append(np.array(subject_features))\n",
        "\n",
        "    return np.array(all_features)\n",
        "\n",
        "# Call the function to process all your data\n",
        "X = extract_features(all_data)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "print(f\"Final data shape (X): {X.shape}\")\n",
        "print(f\"Final labels shape (y): {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0PE-QP8Z4Oz",
        "outputId": "cfcdd6b8-2bc7-4096-d8c6-df0182fca58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data shape (X): (84, 3952)\n",
            "Final labels shape (y): (84,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now training the model final step"
      ],
      "metadata": {
        "id": "THUzpYkraEkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Apply MinMaxScaler to scale your data to a small range (0-1)\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reshape data for the CNN (samples, features, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid') # Use 'sigmoid' for binary classification\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The model will train for 20 epochs (cycles through the data)\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icak2xNFaHKG",
        "outputId": "c52ba665-8c58-4404-bfef-e77c45f956a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step - accuracy: 0.5005 - loss: 1.6275 - val_accuracy: 0.5294 - val_loss: 1.2582\n",
            "Epoch 2/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.5080 - loss: 1.1024 - val_accuracy: 0.4706 - val_loss: 0.8159\n",
            "Epoch 3/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.5339 - loss: 0.6958 - val_accuracy: 0.5294 - val_loss: 1.1665\n",
            "Epoch 4/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5226 - loss: 1.2003 - val_accuracy: 0.5294 - val_loss: 0.9133\n",
            "Epoch 5/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5375 - loss: 0.8057 - val_accuracy: 0.4706 - val_loss: 0.8445\n",
            "Epoch 6/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.4419 - loss: 0.8120 - val_accuracy: 0.4706 - val_loss: 0.7469\n",
            "Epoch 7/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.6405 - loss: 0.6125 - val_accuracy: 0.5294 - val_loss: 0.7159\n",
            "Epoch 8/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.6377 - loss: 0.5463 - val_accuracy: 0.5294 - val_loss: 0.7020\n",
            "Epoch 9/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.5417 - loss: 0.5821 - val_accuracy: 0.4706 - val_loss: 0.9872\n",
            "Epoch 10/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.4924 - loss: 0.7401 - val_accuracy: 0.5294 - val_loss: 0.6868\n",
            "Epoch 11/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 0.6686 - loss: 0.5194 - val_accuracy: 0.5294 - val_loss: 1.1982\n",
            "Epoch 12/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.5606 - loss: 0.8498 - val_accuracy: 0.5294 - val_loss: 0.6830\n",
            "Epoch 13/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.8327 - loss: 0.4524 - val_accuracy: 0.4706 - val_loss: 0.9171\n",
            "Epoch 14/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - accuracy: 0.6107 - loss: 0.5856 - val_accuracy: 0.5294 - val_loss: 0.6669\n",
            "Epoch 15/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.8934 - loss: 0.3689 - val_accuracy: 0.5294 - val_loss: 0.8196\n",
            "Epoch 16/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.7446 - loss: 0.4274 - val_accuracy: 0.5882 - val_loss: 0.6869\n",
            "Epoch 17/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9119 - loss: 0.3489 - val_accuracy: 0.5882 - val_loss: 0.7654\n",
            "Epoch 18/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7176 - loss: 0.4368 - val_accuracy: 0.3529 - val_loss: 0.6545\n",
            "Epoch 19/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9275 - loss: 0.3172 - val_accuracy: 0.5294 - val_loss: 0.9042\n",
            "Epoch 20/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7442 - loss: 0.4022 - val_accuracy: 0.5294 - val_loss: 0.8530\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b01c4136780>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model is overfitted now using the techique of early stopping here"
      ],
      "metadata": {
        "id": "I3NfFAoBap4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5), # Add dropout to prevent overfitting\n",
        "\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the new architecture\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE4YnRObavyq",
        "outputId": "b6acd437-3fd8-450f-df94-cef80023b11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 607ms/step - accuracy: 0.4810 - loss: 3.4320 - val_accuracy: 0.4706 - val_loss: 0.8612\n",
            "Epoch 2/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405ms/step - accuracy: 0.6526 - loss: 3.2071 - val_accuracy: 0.5294 - val_loss: 0.7163\n",
            "Epoch 3/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - accuracy: 0.6639 - loss: 7.1235 - val_accuracy: 0.5294 - val_loss: 0.6889\n",
            "Epoch 4/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - accuracy: 0.8281 - loss: 1.5862 - val_accuracy: 0.4706 - val_loss: 0.6955\n",
            "Epoch 5/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 0.7901 - loss: 1.9569 - val_accuracy: 0.5882 - val_loss: 0.7106\n",
            "Epoch 6/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy: 0.8203 - loss: 1.6818 - val_accuracy: 0.4706 - val_loss: 0.7745\n",
            "Epoch 7/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - accuracy: 0.9005 - loss: 0.6223 - val_accuracy: 0.4706 - val_loss: 0.9326\n",
            "Epoch 8/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - accuracy: 0.8973 - loss: 0.8799 - val_accuracy: 0.4706 - val_loss: 1.1323\n",
            "Epoch 9/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - accuracy: 0.9083 - loss: 0.4200 - val_accuracy: 0.4706 - val_loss: 1.2560\n",
            "Epoch 10/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - accuracy: 0.9694 - loss: 0.4841 - val_accuracy: 0.5294 - val_loss: 1.4431\n",
            "Epoch 11/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - accuracy: 0.9616 - loss: 0.2201 - val_accuracy: 0.5294 - val_loss: 1.6567\n",
            "Epoch 12/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step - accuracy: 0.9503 - loss: 0.1894 - val_accuracy: 0.5294 - val_loss: 1.9049\n",
            "Epoch 13/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 283ms/step - accuracy: 0.9233 - loss: 0.1691 - val_accuracy: 0.5294 - val_loss: 2.1845\n",
            "Epoch 14/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - accuracy: 0.9581 - loss: 0.1317 - val_accuracy: 0.5294 - val_loss: 2.5294\n",
            "Epoch 15/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - accuracy: 0.9886 - loss: 0.0187 - val_accuracy: 0.5294 - val_loss: 2.8433\n",
            "Epoch 16/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - accuracy: 0.9734 - loss: 0.0513 - val_accuracy: 0.5294 - val_loss: 3.1752\n",
            "Epoch 17/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - accuracy: 0.9734 - loss: 0.0745 - val_accuracy: 0.5294 - val_loss: 3.6072\n",
            "Epoch 18/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - accuracy: 0.9886 - loss: 0.0590 - val_accuracy: 0.5294 - val_loss: 3.9724\n",
            "Epoch 19/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - accuracy: 0.9503 - loss: 0.1353 - val_accuracy: 0.5294 - val_loss: 4.2993\n",
            "Epoch 20/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - accuracy: 1.0000 - loss: 7.2379e-04 - val_accuracy: 0.5294 - val_loss: 4.6029\n",
            "Epoch 21/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - accuracy: 0.9698 - loss: 0.1049 - val_accuracy: 0.5294 - val_loss: 4.8585\n",
            "Epoch 22/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 0.5294 - val_loss: 5.0312\n",
            "Epoch 23/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403ms/step - accuracy: 0.9886 - loss: 0.0589 - val_accuracy: 0.5294 - val_loss: 5.1810\n",
            "Epoch 24/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step - accuracy: 0.9506 - loss: 0.1703 - val_accuracy: 0.5294 - val_loss: 5.3448\n",
            "Epoch 25/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 272ms/step - accuracy: 0.9925 - loss: 0.0082 - val_accuracy: 0.5294 - val_loss: 5.5336\n",
            "Epoch 26/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.5294 - val_loss: 5.7522\n",
            "Epoch 27/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.5294 - val_loss: 5.9645\n",
            "Epoch 28/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - accuracy: 0.9123 - loss: 0.4076 - val_accuracy: 0.5294 - val_loss: 6.0979\n",
            "Epoch 29/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.9773 - loss: 0.0459 - val_accuracy: 0.5294 - val_loss: 6.1641\n",
            "Epoch 30/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - accuracy: 0.9886 - loss: 0.0392 - val_accuracy: 0.5294 - val_loss: 6.2048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b01c4716510>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "again the model is overfitted now using simpler model for this"
      ],
      "metadata": {
        "id": "CN1PY5T0bI1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7teo-V2nbNjC",
        "outputId": "4c321bcf-1fd2-4c26-b463-ba61cd83fabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - accuracy: 0.5265 - loss: 0.7613 - val_accuracy: 0.5294 - val_loss: 0.6831\n",
            "Epoch 2/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4885 - loss: 0.6880 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 3/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4696 - loss: 0.6932 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 4/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4852 - loss: 0.6932 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 5/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4579 - loss: 0.6932 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 6/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4735 - loss: 0.6932 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 7/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4892 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 8/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5538 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 9/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5538 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 10/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5148 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 11/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5538 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 12/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5577 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 13/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5265 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 14/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5616 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 15/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5499 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 16/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5499 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 17/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5382 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 18/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5226 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 19/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5499 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 20/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5382 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 21/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5304 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 22/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5187 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 23/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5148 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 24/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5226 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 25/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5460 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 26/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5069 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 27/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5499 - loss: 0.6926 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 28/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5694 - loss: 0.6924 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 29/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5382 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 30/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5265 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6928\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b01c4762060>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "end\n"
      ],
      "metadata": {
        "id": "dQykz0Prbndc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "re doing with the model"
      ],
      "metadata": {
        "id": "HxlCnIKzbo_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.fftpack import rfft, fftfreq\n",
        "from scipy.stats import skew, kurtosis\n",
        "import pywt\n",
        "\n",
        "# Your data loading and reshaping from before\n",
        "# Make sure to run the code that creates `all_data` and `all_labels` first.\n",
        "# Example:\n",
        "# all_data = [...]\n",
        "# all_labels = [...]\n",
        "\n",
        "# --- UPDATED Feature Extraction Function ---\n",
        "def extract_robust_features(eeg_data, n_channels=16, sample_rate=128):\n",
        "    all_features = []\n",
        "    # Loop through each subject's data in the list\n",
        "    for subject_data in eeg_data:\n",
        "        subject_features = []\n",
        "        # Loop through each of the 16 channels\n",
        "        for channel_data in subject_data.T:\n",
        "\n",
        "            # --- Time-Domain Statistical Features ---\n",
        "            subject_features.append(np.mean(channel_data))\n",
        "            subject_features.append(np.std(channel_data))\n",
        "            subject_features.append(skew(channel_data))\n",
        "            subject_features.append(kurtosis(channel_data))\n",
        "\n",
        "            # --- Frequency-Domain Features (Power in different bands) ---\n",
        "            N = len(channel_data)\n",
        "            yf = rfft(channel_data)\n",
        "            xf = fftfreq(N, 1 / sample_rate)\n",
        "\n",
        "            # Define frequency bands\n",
        "            bands = {\n",
        "                'delta': (1, 4),\n",
        "                'theta': (4, 8),\n",
        "                'alpha': (8, 12),\n",
        "                'beta': (12, 30),\n",
        "                'gamma': (30, 45)\n",
        "            }\n",
        "\n",
        "            for band_name, (low, high) in bands.items():\n",
        "                band_power = np.sum(np.abs(yf[(xf > low) & (xf < high)]))\n",
        "                subject_features.append(band_power)\n",
        "\n",
        "            # --- Wavelet Transform Features (Decomposition) ---\n",
        "            # Using the same approach as before\n",
        "            coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n",
        "            subject_features.extend(coeffs[0])\n",
        "\n",
        "        all_features.append(np.array(subject_features))\n",
        "\n",
        "    return np.array(all_features)\n",
        "\n",
        "# Call the new function to process all your data\n",
        "X = extract_robust_features(all_data)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "print(f\"Final data shape (X): {X.shape}\")\n",
        "print(f\"Final labels shape (y): {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6a-emiVbrDM",
        "outputId": "f784c832-693a-46f3-bdf2-725dc63b516c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data shape (X): (84, 4080)\n",
            "Final labels shape (y): (84,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainignt he deep learning model again"
      ],
      "metadata": {
        "id": "pniYcABjb3Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Apply MinMaxScaler to scale your data to a small range (0-1)\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reshape data for the CNN (samples, features, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build a more robust CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The model will train for 30 epochs\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCHX6aqTb66Q",
        "outputId": "1a46ee34-543a-4f38-9c95-f9eaa0e0a13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 435ms/step - accuracy: 0.5304 - loss: 11.2725 - val_accuracy: 0.5294 - val_loss: 0.6526\n",
            "Epoch 2/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 301ms/step - accuracy: 0.6952 - loss: 3.2711 - val_accuracy: 0.4706 - val_loss: 0.7035\n",
            "Epoch 3/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - accuracy: 0.5105 - loss: 3.4199 - val_accuracy: 0.4706 - val_loss: 0.6986\n",
            "Epoch 4/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - accuracy: 0.6718 - loss: 2.1636 - val_accuracy: 0.5294 - val_loss: 0.7050\n",
            "Epoch 5/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 453ms/step - accuracy: 0.6302 - loss: 2.1849 - val_accuracy: 0.5294 - val_loss: 0.8836\n",
            "Epoch 6/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 295ms/step - accuracy: 0.6981 - loss: 0.9100 - val_accuracy: 0.4706 - val_loss: 1.1922\n",
            "Epoch 7/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 295ms/step - accuracy: 0.7709 - loss: 0.6327 - val_accuracy: 0.4706 - val_loss: 1.5894\n",
            "Epoch 8/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - accuracy: 0.8203 - loss: 0.4611 - val_accuracy: 0.4706 - val_loss: 1.8958\n",
            "Epoch 9/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - accuracy: 0.8970 - loss: 0.1827 - val_accuracy: 0.4706 - val_loss: 2.1872\n",
            "Epoch 10/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - accuracy: 0.8359 - loss: 0.2867 - val_accuracy: 0.4706 - val_loss: 2.4878\n",
            "Epoch 11/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 0.9005 - loss: 0.5777 - val_accuracy: 0.4706 - val_loss: 2.9289\n",
            "Epoch 12/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - accuracy: 0.9197 - loss: 0.1231 - val_accuracy: 0.4706 - val_loss: 3.3605\n",
            "Epoch 13/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy: 0.9123 - loss: 0.1900 - val_accuracy: 0.4706 - val_loss: 3.7137\n",
            "Epoch 14/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405ms/step - accuracy: 0.9694 - loss: 0.1605 - val_accuracy: 0.4706 - val_loss: 4.0882\n",
            "Epoch 15/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 458ms/step - accuracy: 0.9389 - loss: 0.1657 - val_accuracy: 0.4706 - val_loss: 4.4456\n",
            "Epoch 16/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 457ms/step - accuracy: 0.9005 - loss: 0.1484 - val_accuracy: 0.4706 - val_loss: 4.8071\n",
            "Epoch 17/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357ms/step - accuracy: 0.9162 - loss: 0.1219 - val_accuracy: 0.4706 - val_loss: 5.1697\n",
            "Epoch 18/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 295ms/step - accuracy: 0.9392 - loss: 0.1506 - val_accuracy: 0.4706 - val_loss: 5.5081\n",
            "Epoch 19/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279ms/step - accuracy: 1.0000 - loss: 0.0569 - val_accuracy: 0.4706 - val_loss: 5.8259\n",
            "Epoch 20/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - accuracy: 0.9197 - loss: 0.1188 - val_accuracy: 0.4706 - val_loss: 6.1511\n",
            "Epoch 21/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 0.9389 - loss: 0.1241 - val_accuracy: 0.4706 - val_loss: 6.4747\n",
            "Epoch 22/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - accuracy: 0.9425 - loss: 0.1034 - val_accuracy: 0.4706 - val_loss: 6.7848\n",
            "Epoch 23/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - accuracy: 0.9581 - loss: 0.0751 - val_accuracy: 0.4706 - val_loss: 7.1169\n",
            "Epoch 24/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.9808 - loss: 0.0589 - val_accuracy: 0.4706 - val_loss: 7.5429\n",
            "Epoch 25/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 452ms/step - accuracy: 1.0000 - loss: 0.0407 - val_accuracy: 0.4706 - val_loss: 7.9286\n",
            "Epoch 26/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 476ms/step - accuracy: 0.9886 - loss: 0.0391 - val_accuracy: 0.4706 - val_loss: 8.2911\n",
            "Epoch 27/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 276ms/step - accuracy: 0.9581 - loss: 0.0794 - val_accuracy: 0.4706 - val_loss: 8.6564\n",
            "Epoch 28/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.9808 - loss: 0.0385 - val_accuracy: 0.4706 - val_loss: 8.9923\n",
            "Epoch 29/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 0.9808 - loss: 0.0718 - val_accuracy: 0.4706 - val_loss: 9.3148\n",
            "Epoch 30/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.9886 - loss: 0.0216 - val_accuracy: 0.4706 - val_loss: 9.6178\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b01c43a31a0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.fftpack import rfft, fftfreq\n",
        "import pywt\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(eeg_data, n_channels=16, sample_rate=128):\n",
        "    all_features = []\n",
        "    # Loop through each subject's data\n",
        "    for subject_data in eeg_data:\n",
        "        subject_features = []\n",
        "        # Loop through each of the 16 channels\n",
        "        for channel_data in subject_data.T:\n",
        "\n",
        "            # --- Fourier Transform Features (Frequency Power) ---\n",
        "            N = len(channel_data)\n",
        "            yf = rfft(channel_data)\n",
        "            xf = fftfreq(N, 1 / sample_rate)\n",
        "            # Example: Get the power of the Alpha band (8-12 Hz)\n",
        "            alpha_power = np.sum(np.abs(yf[(xf > 8) & (xf < 12)]))\n",
        "            subject_features.append(alpha_power)\n",
        "\n",
        "            # --- Wavelet Transform Features (Decomposition) ---\n",
        "            # Decompose the signal using a wavelet\n",
        "            coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n",
        "            # Use the approximation coefficients as a feature\n",
        "            subject_features.extend(coeffs[0])\n",
        "\n",
        "        all_features.append(np.array(subject_features))\n",
        "\n",
        "    return np.array(all_features)\n",
        "\n",
        "# This is the reshaped data from your previous step\n",
        "# You will need to apply the cleaning and reshaping to all your files first\n",
        "# before running this step.\n",
        "# For now, you can test it on your single reshaped_data array\n",
        "X_features = extract_features(reshaped_data[np.newaxis, :])\n",
        "\n",
        "print(f\"Shape of extracted features: {X_features.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNF2w5r3Vo-1",
        "outputId": "df198f90-7b60-4c90-f6b4-5ad47d1d8d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of extracted features: (1, 3952)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.fftpack import rfft, fftfreq\n",
        "import pywt\n",
        "\n",
        "# Define the paths to your data folders\n",
        "schizophrenia_dir = '/content/Schizophrenia/sch/'\n",
        "normal_dir = '/content/Schizophrenia/norm/'\n",
        "\n",
        "# --- Your feature extraction function from before ---\n",
        "def extract_features(eeg_data, n_channels=16, sample_rate=128):\n",
        "    all_features = []\n",
        "    for subject_data in eeg_data:\n",
        "        subject_features = []\n",
        "        for channel_data in subject_data.T:\n",
        "            N = len(channel_data)\n",
        "            yf = rfft(channel_data)\n",
        "            xf = fftfreq(N, 1 / sample_rate)\n",
        "            alpha_power = np.sum(np.abs(yf[(xf > 8) & (xf < 12)]))\n",
        "            subject_features.append(alpha_power)\n",
        "            coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n",
        "            subject_features.extend(coeffs[0])\n",
        "        all_features.append(np.array(subject_features))\n",
        "    return np.array(all_features)\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Create lists to store data and labels for all subjects\n",
        "all_subject_features = []\n",
        "all_subject_labels = []\n",
        "n_channels = 16\n",
        "\n",
        "# Process Schizophrenia files\n",
        "for filename in os.listdir(schizophrenia_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(schizophrenia_dir, filename)\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "        # Reshape data with an auto-calculated number of samples\n",
        "        reshaped_data = df.values.reshape(-1, n_channels)\n",
        "        # Extract features and append to lists\n",
        "        features = extract_features(reshaped_data[np.newaxis, :])\n",
        "        all_subject_features.append(features[0])\n",
        "        all_subject_labels.append(1)\n",
        "\n",
        "# Process Normal files\n",
        "for filename in os.listdir(normal_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(normal_dir, filename)\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "        reshaped_data = df.values.reshape(-1, n_channels)\n",
        "        features = extract_features(reshaped_data[np.newaxis, :])\n",
        "        all_subject_features.append(features[0])\n",
        "        all_subject_labels.append(0)\n",
        "\n",
        "# Convert lists to NumPy arrays for the model\n",
        "X = np.array(all_subject_features)\n",
        "y = np.array(all_subject_labels)\n",
        "\n",
        "print(f\"Final data shape (X): {X.shape}\")\n",
        "print(f\"Final labels shape (y): {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuFyy-tQV7li",
        "outputId": "073bc654-226f-4c60-f35d-ca4168b73100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data shape (X): (84, 3952)\n",
            "Final labels shape (y): (84,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reshape data for the CNN (samples, features, 1)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWTtyfhxWS6Z",
        "outputId": "33326f74-0953-47a8-ec46-b74270bc2fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - accuracy: 0.4735 - loss: 299143.3125 - val_accuracy: 0.4706 - val_loss: 404914.7188\n",
            "Epoch 2/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.4966 - loss: 225117.8438 - val_accuracy: 0.5294 - val_loss: 551.1238\n",
            "Epoch 3/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.5304 - loss: 636.8221 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 4/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.5304 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 5/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5304 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6931\n",
            "Epoch 6/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.5538 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 7/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5187 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 8/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.5343 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 9/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.4991 - loss: 0.6931 - val_accuracy: 0.5294 - val_loss: 0.6930\n",
            "Epoch 10/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.5460 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 11/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 0.5304 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 12/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.5226 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 13/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.5304 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 14/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5577 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 15/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223ms/step - accuracy: 0.5460 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 16/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.5499 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 17/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.5382 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 18/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.5773 - loss: 0.6923 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 19/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 0.5304 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 20/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.5538 - loss: 0.6926 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 21/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.5538 - loss: 0.6926 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 22/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.4991 - loss: 0.6932 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 23/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.5187 - loss: 0.6930 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 24/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.5577 - loss: 0.6925 - val_accuracy: 0.5294 - val_loss: 0.6928\n",
            "Epoch 25/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.5538 - loss: 0.6926 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 26/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5538 - loss: 0.6926 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 27/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.5460 - loss: 0.6927 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 28/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5226 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 29/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5421 - loss: 0.6928 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 30/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.5304 - loss: 0.6929 - val_accuracy: 0.5294 - val_loss: 0.6929\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b0233796480>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The shapes from your previous step are used here\n",
        "num_samples = X_scaled.shape[0]\n",
        "num_features = X_scaled.shape[1]\n",
        "\n",
        "# Reshape the entire dataset first\n",
        "X_reshaped_for_cnn = X_scaled.reshape(num_samples, num_features, 1)\n",
        "\n",
        "# Now, split the reshaped data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_reshaped_for_cnn, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Check the final shapes before feeding to the model\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "zWx1FMpcWXpF",
        "outputId": "87fa6bca-4a0d-4d60-efbc-00edb302ea87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [1, 84]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4089542035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Now, split the reshaped data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_reshaped_for_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 84]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3rd try**"
      ],
      "metadata": {
        "id": "er5zCMMqeZ9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.fftpack import rfft, fftfreq\n",
        "from scipy.stats import skew, kurtosis\n",
        "import pywt\n",
        "\n",
        "# Define the paths to your data folders\n",
        "schizophrenia_dir = '/content/Schizophrenia/sch/'\n",
        "normal_dir = '/content/Schizophrenia/norm/'\n",
        "\n",
        "# Define the number of channels and sample rate\n",
        "n_channels = 16\n",
        "sample_rate = 128\n",
        "\n",
        "# Create empty lists to store all the data and labels\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "# --- Loop through schizophrenia files ---\n",
        "for filename in os.listdir(schizophrenia_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(schizophrenia_dir, filename)\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "        reshaped_data = df.values.reshape(-1, n_channels)\n",
        "        all_data.append(reshaped_data)\n",
        "        all_labels.append(1)  # Label for schizophrenia\n",
        "\n",
        "# --- Loop through normal files ---\n",
        "for filename in os.listdir(normal_dir):\n",
        "    if filename.endswith('.eea'):\n",
        "        file_path = os.path.join(normal_dir, filename)\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "        reshaped_data = df.values.reshape(-1, n_channels)\n",
        "        all_data.append(reshaped_data)\n",
        "        all_labels.append(0)  # Label for normal\n",
        "\n",
        "def extract_robust_features(eeg_data):\n",
        "    all_features = []\n",
        "    for subject_data in eeg_data:\n",
        "        subject_features = []\n",
        "        for channel_data in subject_data.T:\n",
        "            # Time-Domain Statistical Features\n",
        "            subject_features.append(np.mean(channel_data))\n",
        "            subject_features.append(np.std(channel_data))\n",
        "            subject_features.append(skew(channel_data))\n",
        "            subject_features.append(kurtosis(channel_data))\n",
        "\n",
        "            # Frequency-Domain Features\n",
        "            N = len(channel_data)\n",
        "            yf = rfft(channel_data)\n",
        "            xf = fftfreq(N, 1 / sample_rate)\n",
        "            bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 45)}\n",
        "            for low, high in bands.values():\n",
        "                band_power = np.sum(np.abs(yf[(xf > low) & (xf < high)]))\n",
        "                subject_features.append(band_power)\n",
        "\n",
        "            # Wavelet Features\n",
        "            coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n",
        "            subject_features.extend(coeffs[0])\n",
        "\n",
        "        all_features.append(np.array(subject_features))\n",
        "\n",
        "    return np.array(all_features)\n",
        "\n",
        "# Extract features from all processed data\n",
        "X = extract_robust_features(all_data)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "print(f\"Final data shape (X): {X.shape}\")\n",
        "print(f\"Final labels shape (y): {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJOvnEcZW6oM",
        "outputId": "bc3bd3ca-dd39-420c-9d0d-d5b70df34367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data shape (X): (84, 4080)\n",
            "Final labels shape (y): (84,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale the data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size=0.2 means 20% of the data will be used for testing\n",
        "# random_state ensures the split is the same every time you run the code\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\n--- After Splitting ---\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Optional: Check the label distribution\n",
        "print(\"\\n--- Label Distribution ---\")\n",
        "print(\"Training labels:\", np.bincount(y_train))\n",
        "print(\"Testing labels:\", np.bincount(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrzz7U6IeLT4",
        "outputId": "7ea0fa9c-69b2-4cdc-e2ff-969bfdc15245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- After Splitting ---\n",
            "X_train shape: (67, 4080)\n",
            "y_train shape: (67,)\n",
            "X_test shape: (17, 4080)\n",
            "y_test shape: (17,)\n",
            "\n",
            "--- Label Distribution ---\n",
            "Training labels: [31 36]\n",
            "Testing labels: [8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "# We'll use 100 trees in the forest\n",
        "# random_state ensures you get the same results every time\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on your training data\n",
        "# This is where the model learns to find patterns\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "# The model tries to classify the data it has never seen before\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "# We'll use accuracy and a full classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxTYI-C_eRyp",
        "outputId": "854529a2-ba54-4b14-83f4-e4feaeb60a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4117647058823529\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.25      0.29         8\n",
            "           1       0.45      0.56      0.50         9\n",
            "\n",
            "    accuracy                           0.41        17\n",
            "   macro avg       0.39      0.40      0.39        17\n",
            "weighted avg       0.40      0.41      0.40        17\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FprISQu8e0f-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}